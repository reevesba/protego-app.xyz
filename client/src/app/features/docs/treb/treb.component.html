<div class="protego-page-container protego-methods">
    <h1>DecisionTree</h1>
    <p>A decision tree classifier.</p>

    <h2 class="protego-bottom-border">Parameters</h2>
    <ul>
        <li><strong>criterion : {{ '{' }}"gini", "entropy"{{ '}' }}, default="gini"</strong></li>
        <p>The function to measure the quality of a split. Supported criteria are "gini" for the Gini impurity and "entropy" for the information gain.</p>

        <li><strong>splitter : {{ '{' }}"best", "random"{{ '}' }}, default="best"</strong></li>
        <p>The strategy used to choose the split at each node. Supported strategies are "best" to choose the best split and "random" to choose the best random split.</p>

        <li><strong>max_depth : int, default=None</strong></li>
        <p>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>

        <li><strong>min_samples_split : int or float, default=2</strong></li>
        <p>The minimum number of samples required to split an internal node:</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>min_samples_split</code> as the minimum number.</li>
            <li>If float, then <code>min_samples_split</code> is a fraction and <code>ceil(min_samples_split * n_samples)</code> are the minimum number of samples for each split.</li>
        </ul>

        <li><strong>min_samples_leaf : int or float, default=1</strong></li>
        <p>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least <code>min_samples_leaf</code> training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>min_samples_leaf</code> as the minimum number.</li>
            <li>If float, then <code>min_samples_leaf</code> is a fraction and <code>ceil(min_samples_leaf * n_samples)</code> are the minimum number of samples for each node.</li>
        </ul>

        <li><strong>min_weight_fraction_leaf : float, default=0.0</strong></li>
        <p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</p>
    
        <li><strong>max_features : {{ '{' }}"gini", "entropy"{{ '}' }}, default="gini"</strong></li>
        <p>The number of features to consider when looking for the best split:</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>max_features</code> features at each split.</li>
            <li>If float, then <code>max_features</code> is a fraction and <code>int(max_features * n_features)</code> features are considered at each split.</li>
            <li>If "auto", then <code>max_features=sqrt(n_features)</code>.</li>
            <li>If "sqrt", then <code>max_features=sqrt(n_features)</code>.</li>
            <li>If "log2", then <code>max_features=log2(n_features)</code>.</li>
            <li>If None, then <code>max_features=n_features</code>.</li>
        </ul>
        <p>Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than <code>max_features</code> features.</p>

        <li><strong>random_state : int, RandomState instance or None, default=None</strong></li>
        <p>Controls the randomness of the estimator. The features are always randomly permuted at each split, even if <code>splitter</code> is set to <code>"best"</code>. <code>When max_features < n_features</code>, the algorithm will select <code>max_features</code> at random at each split before finding the best split among them. But the best found split may vary across different runs, even if <code>max_features=n_features</code>. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, <code>random_state</code> has to be fixed to an integer.</p>

        <li><strong>max_leaf_nodes : int, default=None</strong></li>
        <p>Grow a tree with <code>max_leaf_nodes</code> in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>

        <li><strong>min_impurity_decrease : float, default=0.0</strong></li>
        <p>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</p>
        <p>The weighted impurity decrease equation is the following:</p>
        <p><code>N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)</code></p>

        <li><strong>class_weight : dict, list of dict or "balanced", default=None</strong></li>
        <p>Weights associated with classes in the form <code>{{ '{' }}class_label: weight{{ '}' }}</code>. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</p>
        <p>Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{{ '{' }}0: 1, 1: 1{{ '}' }}, {{ '{' }}0: 1, 1: 5{{ '}' }}, {{ '{' }}0: 1, 1: 1{{ '}' }}, {{ '{' }}0: 1, 1: 1{{ '}' }}] instead of [{{ '{' }}1:1{{ '}' }}, {{ '{' }}2:5{{ '}' }}, {{ '{' }}3:1{{ '}' }}, {{ '{' }}4:1{{ '}' }}].</p>
        <p>The "balanced" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as <code>n_samples / (n_classes * np.bincount(y))</code>.</p>
        <p>For multi-output, the weights of each column of y will be multiplied.</p>
        <p>Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.</p>

        <li><strong>ccp_alpha : non-negative float, default=0.0</strong></li>
        <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than <code>ccp_alpha</code> will be chosen. By default, no pruning is performed.</p>
    </ul>

    <h2 class="protego-bottom-border">Methods</h2>
    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">save</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Save a model as a pickled file.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>filename: string</strong>
                    <p>Path and filename of pickled model to save.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table> 
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">load</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Load a pickled model.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>filename: string</strong>
                    <p>Path and filename of pickled model to load.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table>  
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">train</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Train a batch classifier.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>train_data: Pandas DataFrame</strong>
                    <p>Samples to use for training classifier.</p>

                    <strong>train_labels: Pandas Series</strong>
                    <p>Training samples labels.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table> 
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">predict</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Predict whether or not samples are benign or malicious.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>data: Pandas DataFrame</strong>
                    <p>Samples to make predictions on.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>Numpy array containing predictions.</td>
            </tr>
        </table>  
    </mat-expansion-panel>

    <h2 class="protego-bottom-border">Examples</h2>
    <div class="protego-editor">
        <button mat-button matTooltip="Toggle Line Numbers" class="protego-editor-btn" (click)="toggleNumbers()"><mat-icon>format_list_numbered</mat-icon></button>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.utils.feature_extractor <span class="protego-key">import</span> FeatureExtractor <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.utils.DataLoader <span class="protego-key">import</span> DataLoader <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.models.batch <span class="protego-key">import</span> DecisionTree <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> sklearn.metrics <span class="protego-key">import</span> accuracy_score <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;</span>data_loader <span class="protego-op">=</span> DataLoader() <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;</span>dataset = data_loader.load() <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-comment"># Split into train/test sets</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;</span>split = <span class="protego-fn">int</span>(<span class="protego-key">len</span>(dataset)*<span class="protego-num">0.8</span>)<br>
        <span *ngIf="showNumbers" class="protego-ln">10&nbsp;&nbsp;&nbsp;&nbsp;</span>train_data = dataset.loc[:split, :]<br>
        <span *ngIf="showNumbers" class="protego-ln">11&nbsp;&nbsp;&nbsp;&nbsp;</span>test_data = dataset.loc[split + <span class="protego-num">1</span>:, :]<br>
        <span *ngIf="showNumbers" class="protego-ln">12&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">13&nbsp;&nbsp;&nbsp;&nbsp;</span>fe = FeatureExtractor() <br>
        <span *ngIf="showNumbers" class="protego-ln">14&nbsp;&nbsp;&nbsp;&nbsp;</span>train_data = fe.transform(train_data) <br>
        <span *ngIf="showNumbers" class="protego-ln">15&nbsp;&nbsp;&nbsp;&nbsp;</span>test_data = fe.transform(test_data) <br>
        <span *ngIf="showNumbers" class="protego-ln">16&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">17&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-comment"># Process dataset for training/testing</span><br>
        <span *ngIf="showNumbers" class="protego-ln">18&nbsp;&nbsp;&nbsp;&nbsp;</span>features = train_data.columns[:-<span class="protego-num">1</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">19&nbsp;&nbsp;&nbsp;&nbsp;</span>train_set = train_data[features]<br>
        <span *ngIf="showNumbers" class="protego-ln">20&nbsp;&nbsp;&nbsp;&nbsp;</span>train_lbl = train_data[<span class="protego-str">"label"</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">21&nbsp;&nbsp;&nbsp;&nbsp;</span>test_set = test_data[features]<br>
        <span *ngIf="showNumbers" class="protego-ln">22&nbsp;&nbsp;&nbsp;&nbsp;</span>test_lbl = test_data[<span class="protego-str">"label"</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">23&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">24&nbsp;&nbsp;&nbsp;&nbsp;</span>clf = DecisionTree() <br>
        <span *ngIf="showNumbers" class="protego-ln">25&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.train(train_set, train_lbl) <br>
        <span *ngIf="showNumbers" class="protego-ln">26&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.save(<span class="protego-str">"my_model_treb.pkl"</span>) <br>
        <span *ngIf="showNumbers" class="protego-ln">27&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.load(<span class="protego-str">"my_model_treb.pkl"</span>) <br>
        <span *ngIf="showNumbers" class="protego-ln">28&nbsp;&nbsp;&nbsp;&nbsp;</span>y_pred = clf.predict(test_set) <br>
        <span *ngIf="showNumbers" class="protego-ln">29&nbsp;&nbsp;&nbsp;&nbsp;</span>print(<span class="protego-key">f</span><span class="protego-str">"Accuracy:</span> <span class="protego-const">{{ '{' }}</span><span class="protego-num">100</span>*accuracy_score(test_lbl, y_pred)<span class="protego-num">:3.3f</span><span class="protego-const">{{ '}' }}</span><span class="protego-str">%"</span>) <br>
    </div>
</div>