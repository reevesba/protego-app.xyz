<div class="protego-page-container protego-methods">
    <h1>RandomForest</h1>
    <p>A random forest classifier.</p>

    <h2 class="protego-bottom-border">Parameters</h2>
    <ul>
        <li><strong>n_estimators : int, default=100</strong></li>
        <p>The number of trees in the forest.</p>

        <li><strong>criterion : {{ '{' }}"gini", "entropy"{{ '}' }}, default="gini"</strong></li>
        <p>The function to measure the quality of a split. Supported criteria are "gini" for the Gini impurity and "entropy" for the information gain. Note: this parameter is tree-specific.</p>

        <li><strong>max_depth : int, default=None</strong></li>
        <p>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>

        <li><strong>min_samples_split : int or float, default=2</strong></li>
        <p>The minimum number of samples required to split an internal node:</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>min_samples_split</code> as the minimum number.</li>
            <li>If float, then <code>min_samples_split</code> is a fraction and <code>ceil(min_samples_split * n_samples)</code> are the minimum number of samples for each split.</li>
        </ul>

        <li><strong>min_samples_leaf : int or float, default=1</strong></li>
        <p>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least <code>min_samples_leaf</code> training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>min_samples_leaf</code> as the minimum number.</li>
            <li>If float, then <code>min_samples_leaf</code> is a fraction and <code>ceil(min_samples_leaf * n_samples)</code> are the minimum number of samples for each node.</li>
        </ul>

        <li><strong>min_weight_fraction_leaf : float, default=0.0</strong></li>
        <p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</p>

        <li><strong>max_features : {{ '{' }}"auto", "sqrt", "log2"{{ '}' }}, int or float, default="auto"</strong></li>
        <p>The number of features to consider when looking for the best split:</p>
        <ul style="margin-bottom: 1em;">
            <li>If int, then consider <code>max_features</code> features at each split.</li>
            <li>If float, then <code>max_features</code> is a fraction and <code>round(max_features * n_features)</code> features are considered at each split.</li>
            <li>If "auto", then <code>max_features=sqrt(n_features)</code>.</li>
            <li>If "sqrt", then <code>max_features=sqrt(n_features)</code> (same as "auto").</li>
            <li>If "log2", then <code>max_features=log2(n_features)</code>.</li>
            <li>If None, then <code>max_features=n_features</code>.</li>
        </ul>

        <li><strong>max_leaf_nodes : int, default=None</strong></li>
        <p>Grow trees with <code>max_leaf_nodes</code> in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>

        <li><strong>min_impurity_decrease : float, default=0.0</strong></li>
        <p>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</p>
        <p>The weighted impurity decrease equation is the following</p>
        <div style="margin-bottom: 1em;"><code>N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)</code></div>
        <p>where <code>N</code> is the total number of samples, <code>N_t</code> is the number of samples at the current node, <code>N_t_L</code> is the number of samples in the left child, and <code>N_t_R</code> is the number of samples in the right child.</p>
        <p><code>N</code>, <code>N_t</code>, <code>N_t_R</code> and <code>N_t_L</code> all refer to the weighted sum, if <code>sample_weight</code> is passed.</p>

        <li><strong>bootstrap : bool, default=True</strong></li>
        <p>Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.</p>

        <li><strong>oob_score : bool, default=False</strong></li>
        <p>Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.</p>

        <li><strong>n_jobs : int, default=None</strong></li>
        <p>The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. <code>None</code> means 1 unless in a joblib.parallel_backend context. <code>-1</code> means using all processors.</p>

        <li><strong>random_state : int, RandomState instance or None, default=None</strong></li>
        <p>Controls both the randomness of the bootstrapping of the samples used when building trees (if <code>bootstrap=True</code>) and the sampling of the features to consider when looking for the best split at each node (if <code>max_features < n_features</code>).</p>

        <li><strong>verbose : int, default=0</strong></li>
        <p>Controls the verbosity when fitting and predicting.</p>

        <li><strong>warm_start : bool, default=False</strong></li>
        <p>When set to <code>True</code>, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.</p>

        <li><strong>class_weight : {{ '{' }}"balanced", "balanced_subsample"{{ '}' }}, dict or list of dicts, default=None</strong></li>
        <p>Weights associated with classes in the form <code>{{ '{' }}class_label: weight{{ '}' }}</code>. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</p>
        <p>Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{{ '{' }}0: 1, 1: 1{{ '}' }}, {{ '{' }}0: 1, 1: 5{{ '}' }}, {{ '{' }}0: 1, 1: 1{{ '}' }}, {{ '{' }}0: 1, 1: 1{{ '}' }}] instead of [{{ '{' }}1:1{{ '}' }}, {{ '{' }}2:5{{ '}' }}, {{ '{' }}3:1{{ '}' }}, {{ '{' }}4:1{{ '}' }}].</p>
        <p>The "balanced" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as <code>n_samples / (n_classes * np.bincount(y))</code>.</p>
        <p>The "balanced_subsample" mode is the same as "balanced" except that weights are computed based on the bootstrap sample for every tree grown.</p>
        <p>For multi-output, the weights of each column of y will be multiplied.</p>
        <p>Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.</p>

        <li><strong>ccp_alpha : non-negative float, default=0.0</strong></li>
        <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See <a href="https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning" target="_blank">Minimal Cost-Complexity Pruning</a> for details.</p>

        <li><strong>max_samples : int or float, default=None</strong></li>
        <p>If bootstrap is True, the number of samples to draw from X to train each base estimator.</p>
        <ul>
            <li>If None (default), then draw <code>X.shape[0]</code> samples.</li>
            <li>If int, then draw <code>max_samples</code> samples.</li>
            <li>If float, then draw <code>max_samples * X.shape[0]</code> samples. Thus, <code>max_samples</code> should be in the interval <code>(0.0, 1.0]</code>.</li>
        </ul>
    </ul>

    <h2 class="protego-bottom-border">Methods</h2>
    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">save</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Save a model as a pickled file.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>filename: string</strong>
                    <p>Path and filename of pickled model to save.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table> 
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">load</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Load a pickled model.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>filename: string</strong>
                    <p>Path and filename of pickled model to load.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table>  
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">train</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Train the classifier.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>train_data: Pandas DataFrame</strong>
                    <p>Samples to use for training classifier.</p>

                    <strong>train_labels: Pandas Series</strong>
                    <p>Training samples labels.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>None</td>
            </tr>
        </table> 
    </mat-expansion-panel>

    <mat-expansion-panel hideToggle class="mat-elevation-z0">
        <mat-expansion-panel-header>
            <mat-panel-title>
                <mat-icon>info</mat-icon>
                <span style="margin-left: 0.5em;">predict</span>
            </mat-panel-title>
        </mat-expansion-panel-header>
        <p style="margin: 2em 0;">Predict whether or not samples are benign or malicious.</p>
        <table>
            <tr>
                <th>Parameters:</th>
                <td>
                    <strong>data: Pandas DataFrame</strong>
                    <p>Samples to make predictions on.</p>
                </td>
            </tr>
            <tr>
                <th>Returns:</th>
                <td>Numpy array containing predictions.</td>
            </tr>
        </table>  
    </mat-expansion-panel>

    <h2 class="protego-bottom-border">Examples</h2>
    <div class="protego-editor">
        <button mat-button matTooltip="Toggle Line Numbers" class="protego-editor-btn" (click)="toggleNumbers()"><mat-icon>format_list_numbered</mat-icon></button>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.utils.feature_extractor <span class="protego-key">import</span> FeatureExtractor <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.utils.DataLoader <span class="protego-key">import</span> DataLoader <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> protego.models.batch <span class="protego-key">import</span> RandomForest <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-key">from</span> sklearn.metrics <span class="protego-key">import</span> accuracy_score <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;</span>data_loader <span class="protego-op">=</span> DataLoader() <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;</span>dataset = data_loader.load() <br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-comment"># Split into train/test sets</span><br>
        <span *ngIf="showNumbers" class="protego-ln">&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;</span>split = <span class="protego-fn">int</span>(<span class="protego-key">len</span>(dataset)*<span class="protego-num">0.8</span>)<br>
        <span *ngIf="showNumbers" class="protego-ln">10&nbsp;&nbsp;&nbsp;&nbsp;</span>train_data = dataset.loc[:split, :]<br>
        <span *ngIf="showNumbers" class="protego-ln">11&nbsp;&nbsp;&nbsp;&nbsp;</span>test_data = dataset.loc[split + <span class="protego-num">1</span>:, :]<br>
        <span *ngIf="showNumbers" class="protego-ln">12&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">13&nbsp;&nbsp;&nbsp;&nbsp;</span>fe = FeatureExtractor() <br>
        <span *ngIf="showNumbers" class="protego-ln">14&nbsp;&nbsp;&nbsp;&nbsp;</span>train_data = fe.transform(train_data) <br>
        <span *ngIf="showNumbers" class="protego-ln">15&nbsp;&nbsp;&nbsp;&nbsp;</span>test_data = fe.transform(test_data) <br>
        <span *ngIf="showNumbers" class="protego-ln">16&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">17&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="protego-comment"># Process dataset for training/testing</span><br>
        <span *ngIf="showNumbers" class="protego-ln">18&nbsp;&nbsp;&nbsp;&nbsp;</span>features = train_data.columns[:-<span class="protego-num">1</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">19&nbsp;&nbsp;&nbsp;&nbsp;</span>train_set = train_data[features]<br>
        <span *ngIf="showNumbers" class="protego-ln">20&nbsp;&nbsp;&nbsp;&nbsp;</span>train_lbl = train_data[<span class="protego-str">"label"</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">21&nbsp;&nbsp;&nbsp;&nbsp;</span>test_set = test_data[features]<br>
        <span *ngIf="showNumbers" class="protego-ln">22&nbsp;&nbsp;&nbsp;&nbsp;</span>test_lbl = test_data[<span class="protego-str">"label"</span>]<br>
        <span *ngIf="showNumbers" class="protego-ln">23&nbsp;&nbsp;&nbsp;&nbsp;</span><br>
        <span *ngIf="showNumbers" class="protego-ln">24&nbsp;&nbsp;&nbsp;&nbsp;</span>clf = RandomForest() <br>
        <span *ngIf="showNumbers" class="protego-ln">25&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.train(train_set, train_lbl) <br>
        <span *ngIf="showNumbers" class="protego-ln">26&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.save(<span class="protego-str">"my_model_ranb.pkl"</span>) <br>
        <span *ngIf="showNumbers" class="protego-ln">27&nbsp;&nbsp;&nbsp;&nbsp;</span>clf.load(<span class="protego-str">"my_model_ranb.pkl"</span>) <br>
        <span *ngIf="showNumbers" class="protego-ln">28&nbsp;&nbsp;&nbsp;&nbsp;</span>y_pred = clf.predict(test_set) <br>
        <span *ngIf="showNumbers" class="protego-ln">29&nbsp;&nbsp;&nbsp;&nbsp;</span>print(<span class="protego-key">f</span><span class="protego-str">"Accuracy:</span> <span class="protego-const">{{ '{' }}</span><span class="protego-num">100</span>*accuracy_score(test_lbl, y_pred)<span class="protego-num">:3.3f</span><span class="protego-const">{{ '}' }}</span><span class="protego-str">%"</span>) <br>
    </div>
</div>